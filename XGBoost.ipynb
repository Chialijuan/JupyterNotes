{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "Just an advance implementation of GBM\n",
    "https://xgboost.readthedocs.io/en/latest/tutorials/model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective Function: \n",
    "\n",
    "1. Mean Squared Error (Regression)\n",
    "2. Logistic Loss (Classification)\n",
    "3. Cross-entropy (Multi-class classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost is basically boosted trees that incldues:**\n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "1. Regularization\n",
    "    - Standard GBM implementation has no regularization \n",
    "2. Takes in different loss functions and evaluation criteria\n",
    "3. Parallel tree building\n",
    "4. Handles missing data in-built\n",
    "5. Tree Pruning     \n",
    "    - A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a **greedy algorithm.**\n",
    "    - XGBoost on the other hand make **splits upto the max_depth** specified and then start **pruning** the tree backwards and remove splits beyond which there is no positive gain.\n",
    "    - Another advantage is that sometimes a split of negative loss say -2 may be followed by a split of positive loss +10. GBM would stop as it encounters -2. But XGBoost will go deeper and it will see a combined effect of +8 of the split and keep both.\n",
    "6. Built-in CV\n",
    "    - XGBoost can run a CV at each iteration of boosting process, easy to get exact optimum number of boosting iterations in single run\n",
    "    - GBM have to run gridsearch and only limited values can be tested at a time \n",
    "7. Continue on Existing Model\n",
    "    - Both GBM and XGboost have this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost parameters**\n",
    "\n",
    "1. Genreal Parameters\n",
    "\n",
    "\n",
    "   - `booster`\n",
    "       - Type of model to run at each iteration\n",
    "       - _gbtree_: tree-based models\n",
    "       - _gblinear_: linear models\n",
    "       \n",
    "       \n",
    "   - `silent`\n",
    "       - _0_: running messages will be printed\n",
    "       - _1_: no running messages\n",
    "\n",
    "\n",
    "   - `nthread`\n",
    "       - Used for parallel processing\n",
    "       - Default maximum\n",
    "       - Enter number of cores\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Boosting Parameters\n",
    "\n",
    "- `eta`\n",
    "    - Learning rate\n",
    "    - Typical: 0.01~0.2\n",
    "    \n",
    "    \n",
    "- `min_child_weight`\n",
    "    - Control over-fitting, high values = prevent model from learning relations that are highly specific to particular sample selected for a tree\n",
    "    - Similar to GBM's min num. of observations but this is min. sum of weights of observations\n",
    "    \n",
    "    \n",
    "- `max_depth`\n",
    "    - Control over-fitting,  higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Typical: 3-10\n",
    "    - Tune with CV\n",
    "    \n",
    "    \n",
    "- `max_leaf_nodes`\n",
    "    - Similar to `max_depth`\n",
    "    \n",
    "    \n",
    "- `gamma`\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function\n",
    "    - Gamma specifies the minimum loss reduction required to make a split\n",
    "    - Default = 0, higher values = more conservative \n",
    "    \n",
    "- `max_delta_step`\n",
    "- `subsample`\n",
    "- `colsample_bytree`\n",
    "- `colsample_bylevel`\n",
    "- `lambda`\n",
    "- `alpha`\n",
    "- `scale_pos_weight`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Learning Parameters\n",
    "\n",
    "- `objective`\n",
    "- `eval_metric`\n",
    "- `seed`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
