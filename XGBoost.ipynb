{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### XGBoost\n",
    "\n",
    "Just an advance implementation of GBM\n",
    "https://xgboost.readthedocs.io/en/latest/tutorials/model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Objective Function: \n",
    "\n",
    "1. Mean Squared Error (Regression)\n",
    "2. Logistic Loss (Classification)\n",
    "3. Cross-entropy (Multi-class classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**XGBoost is basically boosted trees that incldues:**\n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "1. Regularization\n",
    "    - Standard GBM implementation has no regularization \n",
    "2. Takes in different loss functions and evaluation criteria\n",
    "3. Parallel tree building\n",
    "4. Handles missing data in-built\n",
    "5. Tree Pruning     \n",
    "    - A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a **greedy algorithm.**\n",
    "    - XGBoost on the other hand make **splits upto the max_depth** specified and then start **pruning** the tree backwards and remove splits beyond which there is no positive gain.\n",
    "    - Another advantage is that sometimes a split of negative loss say -2 may be followed by a split of positive loss +10. GBM would stop as it encounters -2. But XGBoost will go deeper and it will see a combined effect of +8 of the split and keep both.\n",
    "6. Built-in CV\n",
    "    - XGBoost can run a CV at each iteration of boosting process, easy to get exact optimum number of boosting iterations in single run\n",
    "    - GBM have to run gridsearch and only limited values can be tested at a time \n",
    "7. Continue on Existing Model\n",
    "    - Both GBM and XGboost have this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### XGBoost parameters\n",
    "\n",
    "**1. General Parameters**\n",
    "\n",
    "\n",
    "   - `booster`\n",
    "       - Type of model to run at each iteration\n",
    "       - _gbtree_: tree-based models\n",
    "       - _gblinear_: linear models\n",
    "       \n",
    "       \n",
    "   - `silent`\n",
    "       - _0_: running messages will be printed\n",
    "       - _1_: no running messages\n",
    "\n",
    "\n",
    "   - `nthread`\n",
    "       - Used for parallel processing\n",
    "       - Default maximum\n",
    "       - Enter number of cores\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**2. Boosting Parameters**\n",
    "\n",
    "- `eta`\n",
    "    - Learning rate\n",
    "    - Typical: 0.01~0.2\n",
    "    \n",
    "    \n",
    "- `min_child_weight` _tree_\n",
    "    - Control over-fitting, high values = prevent model from learning relations that are highly specific to particular sample selected for a tree\n",
    "    - Similar to GBM's min num. of observations but this is min. sum of weights of observations\n",
    "    \n",
    "    \n",
    "- `max_depth` _tree_\n",
    "    - Control over-fitting,  higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Typical: 3-10\n",
    "    - Tune with CV\n",
    "    \n",
    "    \n",
    "- `max_leaf_nodes`\n",
    "    - Similar to `max_depth`\n",
    "    \n",
    "    \n",
    "- `gamma` _tree_\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function\n",
    "    - Gamma specifies the minimum loss reduction required to make a split\n",
    "    - Default = 0, higher values = more conservative \n",
    "    \n",
    "    \n",
    "- `max_delta_step`\n",
    "    - Default = 0, might help in logistic regression with extremely imbalanced class\n",
    "    - Max delta step we allow each tree's weight estimation to be\n",
    "    - 0 = no constraint, higher value = update step more conservative\n",
    "    \n",
    "    \n",
    "- `subsample` _tree_\n",
    "    - Same as subsample in GBM\n",
    "    - Lower values = more conservative, prevent overfitting. Too small = underfitting\n",
    "    - Typical: 0.5-1\n",
    "    \n",
    "    \n",
    "- `colsample_bytree` _tree_\n",
    "    - Similar to max_features in GBM\n",
    "    - Fraction of columns to be randomly sampled for each tree\n",
    "    - Typical: 0.5-1\n",
    "\n",
    "\n",
    "- `colsample_bylevel`\n",
    "    - Subsample ratio of columns for each split, in each level\n",
    "    - Not really as `colsample_bytree` is used\n",
    "    \n",
    "\n",
    "- `reg_alpha` _reg_\n",
    "    - L1 regularization term on weights\n",
    "    - Assign certain weights to 0\n",
    "    - Used in case of very high dimensionality for algo to run faster\n",
    "    \n",
    "    \n",
    "- `reg_lambda` _reg_\n",
    "    - L2 regularizatin term on weights\n",
    "    - Redistribute weights on weights/features\n",
    "    - Explore for reducing overfitting\n",
    "\n",
    "\n",
    "- `scale_pos_weight`\n",
    "    - Value > 0 could help in faster convergence when used in case of high class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**3. Learning Parameters**\n",
    "\n",
    "- `objective`\n",
    "    - Default = 'reg:linear'\n",
    "    - binary: logistic = returns predicted proba\n",
    "    - multi: softmax = returns predicted class\n",
    "    - multi: softprob = returns predicted proba of each point belonging to each class\n",
    "    \n",
    "    \n",
    "- `eval_metric`\n",
    "    - Default to objective\n",
    "    - Regression: rmse, Classification: error\n",
    "    - logloss, mae, merror(multiclass classification error rate), mlogloss(multiclass logloss), auc \n",
    "    \n",
    "    \n",
    "- `seed`\n",
    "    - Fix it for generating reproducible results and parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**xgb: direct xgboost library.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "\n",
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "modelfit(xgb1, train, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['Disbursed'],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Disbursed'].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**XGBClassifier:  allows us to use sklearn’s Grid Search with parallel processing in the same way we did for GBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "gsearch1.fit(train[predictors],train[target])\n",
    "\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**General Approach for Parameter Tuning**\n",
    "\n",
    "1. Relatively high learning rate of 0.05-0.3, determine optimum no. of trees for chosen learning rate\n",
    "2. Tune tree-specific parameters \n",
    "3. Tune regularization parameters\n",
    "4. Lower learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Rule of Thumb for initial**\n",
    "\n",
    "Fix learning rate and number of estimators\n",
    "\n",
    "**2. Tree-specific parameters (tuned first as they have highest impact)**\n",
    "\n",
    "- `max_depth` = 5\n",
    "    - Typical Values: 3-10\n",
    "    \n",
    "    \n",
    "- `min_child_weight` = 1\n",
    "    - Smaller value chosen for highly imbalanced class\n",
    "    \n",
    "    \n",
    "- `gamma` = 0 \n",
    "    - Typical Values = 0.1-0.2\n",
    "    - To be tuned later\n",
    "    \n",
    "    \n",
    "- `subsample` = 0.8\n",
    "    - Typical Values: 0.5-0.9\n",
    "    \n",
    "    \n",
    "- `colsample_bytree` = 0.8\n",
    "    - Typical Values: 0.5-0.9\n",
    "    \n",
    "    \n",
    "- `scale_pos_weight` = 1\n",
    "    - Because of high class imbalance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**3. Regularization parameters**\n",
    "\n",
    "Though many people don’t use this parameters much as gamma provides a substantial way of controlling complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**- Difficult to get a very big leap in performance via parameter tuning**  \n",
    "**- Could be obtained by feature engineering, ensemble models, stacking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaf-wise tree growth compared to level-wise tree growth in XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros:**\n",
    "\n",
    "1. **Faster training speed & efficiency**\n",
    "    - Historgram-based algorithm: bucket continuous feature values into discrete bins which reduces training speed\n",
    "\n",
    "\n",
    "2. **Lower memory usage**\n",
    "    - Above method reduces memory usage\n",
    "\n",
    "\n",
    "3. **Leaf-wise instead of XGBoost's Level-wise splitting**\n",
    "    - Allows for more complex models that leads to higher accuracy\n",
    "    - But could also contributes to overfitting which can be avoided by tuning `max_depth`\n",
    "\n",
    "\n",
    "4. **Good for Large Datasets**\n",
    "\n",
    "\n",
    "5. **Supports parallel learning**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
