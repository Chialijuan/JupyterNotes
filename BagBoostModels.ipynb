{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading in data\n",
    "# Credit-card kaggle\n",
    "cc_df = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "# X: features\n",
    "x_cc = cc_df.iloc[:,1:30]\n",
    "x_cc = np.array(x_cc)\n",
    "\n",
    "# Y: labels\n",
    "y_cc = cc_df['Class'].values\n",
    "\n",
    "# Original\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_cc,\n",
    "    y_cc,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "Decision tree regressor: https://www.saedsayad.com/decision_tree_reg.htm  \n",
    "Decision tree classifier: https://www.saedsayad.com/decision_tree.htm  \n",
    "\n",
    "https://medium.com/@pushkarmandot/how-exactly-xgboost-works-a320d9b8aeef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest:  \n",
    "1. Random sampling of training data points when building trees   \n",
    "(e.g. randomly select with replacement, no. of data points selected same as original [bootstrapping])\n",
    "    - The idea is that by training each tree on different samples, although each tree might have high variance with respect to a particular set of the training data, overall, the entire forest will have lower variance but not at the cost of increasing the bias. At test time, predictions are made by averaging the predictions of each decision tree [aggregating]. (Also known as **bagging**)  \n",
    "    - Typically 1/3 of original data does not make it to the bootstrapped dataset, they are the **Out-of-Bag** data points\n",
    "    - Use **Out-of-bag Error** to choose the parameters (e.g. no. variables to sample) in Random Forest model.\n",
    "    \n",
    "2. Random subsets of features considered when splitting nodes (e.g. randomly selects 4 out of 10 features and use gini to split)\n",
    "\n",
    "3. Dealing with missing data:\n",
    "    - Fill in missing with most common value\n",
    "    - Refine guess by identifying samples that are similar to the one with missing data by _building a RF, run data through all RF, record similar samples (end in same leaf node) in proximity matrix_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pure Decision Tree not flexible when it comes to classifying new samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting algorithms\n",
    "\n",
    "Used for reducing bias and variance in supervised learning by combining multiple weak predictors to build a strong predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "\n",
    "_Uses weight scale to distinguish classified and misclassified data points. High weight = misclassfied_\n",
    "\n",
    "Step 1: Train a decision tree with each observation has equal weights\n",
    "\n",
    "Step 2: Evaluate the first tree, increase the weights of those that were misclassified, decrease those that were correct\n",
    "\n",
    "Step 3: Second tree is trained(grown) on newly weighted data\n",
    "\n",
    "Step 4: Compute classification error from 2-tree ensemble model (Tree1 + Tree2) and grow third tree to predict.\n",
    "\n",
    "Step 5: Iterate\n",
    "\n",
    "_Subsequent trees help to classify observations that are not well classified by previous trees_\n",
    "\n",
    "**Final model: weighted sum of predictions made by previous tree models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "_Uses loss function to measure the degree of misclassification/classification_\n",
    "\n",
    "Step 1 : Assume mean is the prediction of all variables.\n",
    "\n",
    "Step 2 : Calculate errors of each observation from the mean (latest prediction).\n",
    "\n",
    "Step 3 : Find the variable that can split the errors perfectly and find the value for the split. This is assumed to be the latest prediction.\n",
    "\n",
    "Step 4 : Calculate errors of each observation from the mean of both the sides of split (latest prediction).\n",
    "\n",
    "Step 5 : Repeat the step 3 and 4 till the objective function maximizes/minimizes.\n",
    "\n",
    "Step 6 : Take a weighted mean of all the classifiers to come up with the final model.\n",
    "\n",
    "Regression: loss function would be based off the error between true & predicted value\n",
    "Classification: loss function would be a measure of how good the model is at classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Tree Boosting\n",
    "\n",
    "Pros:\n",
    "- Handles data of mixed type (e.g. continuous, discrete, category)\n",
    "- Robust to  outliers via robust loss functions\n",
    "\n",
    "Cons:\n",
    "- Scalability due to sequential nature of boosting that can't be parallelized\n",
    "\n",
    "`GradientBoostingClassifier`: uses decision stumps (decision trees)\n",
    "`GradientBoostingRegressor`: supports a number of loss functions (default: least squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_squared_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-822decb2b524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSE: %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_squared_error' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit Regression model\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf = GradientBoostingRegressor(**params)\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "mse = mean_squared_error(y_test, clf.predict(x_test))\n",
    "print(\"MSE: %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we need feel that the model is underfitting and wants to add more estimators\n",
    "_ = clf.set_params(n_estimators=200, warm_start=True)\n",
    "_.fit(x_train, y_train)\n",
    "mse = mean_squared_error(y_test, clf.predict(x_test))\n",
    "print(\"MSE: %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training deviance\n",
    "\n",
    "# compute test set deviance\n",
    "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(clf.staged_predict(X_test)):\n",
    "    test_score[i] = clf.loss_(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n",
    "         label='Training Set Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "         label='Test Set Deviance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance')\n",
    "\n",
    "# #############################################################################\n",
    "# Plot feature importance\n",
    "feature_importance = clf.feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, boston.feature_names[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out-of-bag estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros:\n",
    "- Allows one to optimize a user-specified cost function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
